# ğŸ§  AI Powered Multimodal Deepfake Detection System

A capstone project developed to detect fake content across multiple modalities â€” **text, image, and video** â€” using fine-tuned deep learning models. This system is designed with the potential to be scaled into a real-world solution for identifying disinformation across various digital platforms.

---

## ğŸš€ Project Overview

This project leverages:
- **BERT** for disinformation detection in captions
- **ViT (Vision Transformer)** for image-based deepfake classification
- **ResNet50** for frame-level video classification

Models are developed and evaluated using Jupyter Notebooks on Google Colab. Final deployment is handled through a Gradio interface that fuses predictions from all three modalities.

---

## ğŸ§± Project Structure

```
AI_Powered_multimodal_Deepfake_Detection_system/
â”œâ”€â”€ Deploy/
â”‚   â”œâ”€â”€ fused_final_gradio.ipynb   # ğŸ”¥ Main interface to launch the app
â”‚   â”œâ”€â”€ app.py                     # Gradio app logic
â”‚   â”œâ”€â”€ Dockerfile                 # Optional: containerization setup
â”‚   â””â”€â”€ requirements.txt           # Deployment dependencies
â”œâ”€â”€ Models_ipynb/
â”‚   â”œâ”€â”€ *.ipynb                    # Training notebooks for BERT, ViT, ResNet50
â”œâ”€â”€ .env.example                   # Sample environment config (AWS keys, S3 paths)
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md                      # ğŸ“„ You're here
```

## ğŸ“¦ Tech Stack
ğŸ§  Transformers (bert-base-uncased)

- ğŸ–¼ï¸ Vision Transformer (ViT) via Hugging Face

- ğŸ¥ ResNet50 for frame-based video analysis

- âš™ï¸ Gradio for deployment

- â˜ï¸ AWS S3 for cloud-based data storage and retrieval

- ğŸ Python, PyTorch, Docker

## ğŸ› ï¸ How to Run
1. Clone the repository:
```
git clone https://github.com/anthonymbah1/AI_Powered_multimodal_Deepfake_Detection_system.git
cd AI_Powered_multimodal_Deepfake_Detection_system
```

2. Set up your environment:

- Create a .env file using .env.example

- Install dependencies using Deploy/requirements.txt

3. Launch the system:

- Open and run **Deploy/fused_final_gradio.ipynb** in Google Colab

## ğŸ“Š Results & Performance
- Multimodal fusion improves robustness and reduces false positives

- Preliminary tests show ~92% accuracy across combined modalities

## ğŸ” Environment Variables
```
Add these to your .env file:
AWS_ACCESS_KEY_ID=your_key_here
AWS_SECRET_ACCESS_KEY=your_secret_here
AWS_REGION=us-east-1
S3_BUCKET_NAME=your-bucket-name
S3_TRAIN_KEY=csvfils/train.csv
S3_VALIDATION_KEY=csvfils/validation.csv
S3_TEST_KEY=csvfils/test.csv
```

## ğŸ‘¨ğŸ½â€ğŸ’» Author
**Tony Mbah**
ğŸ”— [LinkedIn](https://www.linkedin.com/in/tony-mbah)
