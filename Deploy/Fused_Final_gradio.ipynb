{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_wHTfpR-_tcs"
      },
      "outputs": [],
      "source": [
        "# final_Gradio_Interface_Fused.ipynb\n",
        "\n",
        "import gradio as gr\n",
        "import torch\n",
        "import os\n",
        "import csv\n",
        "from datetime import datetime\n",
        "from transformers import ViTForImageClassification, ViTImageProcessor\n",
        "from transformers import BertForSequenceClassification, BertTokenizer\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import timm\n",
        "import torch.nn as nn\n",
        "import cv2\n",
        "from torchvision import transforms\n",
        "\n",
        "# Load models and tokenizers\n",
        "local_model_root = \"./models/\"\n",
        "\n",
        "vit_model = ViTForImageClassification.from_pretrained(os.path.join(local_model_root, \"vit_deepfake_model\")).to('cpu')\n",
        "vit_processor = ViTImageProcessor.from_pretrained(os.path.join(local_model_root, \"vit_deepfake_model\"))\n",
        "\n",
        "bert_original_model = BertForSequenceClassification.from_pretrained(os.path.join(local_model_root, \"bert-original-caption-model\")).to('cpu')\n",
        "bert_original_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "bert_generated_model = BertForSequenceClassification.from_pretrained(os.path.join(local_model_root, \"bert-generated-caption-model\")).to('cpu')\n",
        "bert_generated_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Load video model\n",
        "video_model = timm.create_model('xception', pretrained=False, num_classes=2)\n",
        "video_model.load_state_dict(torch.load(os.path.join(local_model_root, \"video_model.pth\"), map_location=torch.device('cpu')))\n",
        "video_model.eval()\n",
        "\n",
        "vit_model.eval()\n",
        "bert_original_model.eval()\n",
        "bert_generated_model.eval()\n",
        "\n",
        "# Logging function\n",
        "def log_prediction(input_type, label, confidence):\n",
        "    log_file = \"predictions.csv\"\n",
        "    header = ['timestamp', 'input_type', 'label', 'confidence']\n",
        "\n",
        "    file_exists = os.path.isfile(log_file)\n",
        "\n",
        "    with open(log_file, mode='a', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        if not file_exists:\n",
        "            writer.writerow(header)\n",
        "        writer.writerow([datetime.now().isoformat(), input_type, label, f\"{confidence:.2f}\"])\n",
        "\n",
        "# Image prediction function\n",
        "def predict_image(image):\n",
        "    inputs = vit_processor(images=image, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        outputs = vit_model(**inputs)\n",
        "    logits = outputs.logits\n",
        "    probs = torch.nn.functional.softmax(logits, dim=-1)\n",
        "    confidence, prediction = torch.max(probs, dim=1)\n",
        "\n",
        "    label_map = {0: \"Fake\", 1: \"Real\"}\n",
        "    label = label_map[prediction.item()]\n",
        "    confidence_percent = confidence.item() * 100\n",
        "\n",
        "    log_prediction(\"image\", label, confidence_percent)\n",
        "\n",
        "    return f\"Image Prediction: {label} ({confidence_percent:.2f}%)\"\n",
        "\n",
        "# Text prediction function (both original and generated)\n",
        "def predict_both_texts(text):\n",
        "    inputs_original = bert_original_tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "    with torch.no_grad():\n",
        "        outputs_original = bert_original_model(**inputs_original)\n",
        "    logits_original = outputs_original.logits\n",
        "    probs_original = torch.nn.functional.softmax(logits_original, dim=-1)\n",
        "    confidence_original, prediction_original = torch.max(probs_original, dim=1)\n",
        "\n",
        "    label_map = {0: \"Fake News\", 1: \"Real News\"}\n",
        "    label_original = label_map[prediction_original.item()]\n",
        "    confidence_percent_original = confidence_original.item() * 100\n",
        "\n",
        "    inputs_generated = bert_generated_tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "    with torch.no_grad():\n",
        "        outputs_generated = bert_generated_model(**inputs_generated)\n",
        "    logits_generated = outputs_generated.logits\n",
        "    probs_generated = torch.nn.functional.softmax(logits_generated, dim=-1)\n",
        "    confidence_generated, prediction_generated = torch.max(probs_generated, dim=1)\n",
        "\n",
        "    label_generated = label_map[prediction_generated.item()]\n",
        "    confidence_percent_generated = confidence_generated.item() * 100\n",
        "\n",
        "    log_prediction(\"text\", f\"Original: {label_original}, Generated: {label_generated}\", (confidence_percent_original + confidence_percent_generated) / 2)\n",
        "\n",
        "    return (\n",
        "        f\"Original Text Model: {label_original} ({confidence_percent_original:.2f}%)\",\n",
        "        f\"Generated Text Model: {label_generated} ({confidence_percent_generated:.2f}%)\"\n",
        "    )\n",
        "\n",
        "# Video prediction function\n",
        "def extract_frames(video_file, num_frames=16):\n",
        "    frames = []\n",
        "    cap = cv2.VideoCapture(video_file)\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    frame_indices = np.linspace(0, total_frames - 1, num_frames, dtype=int)\n",
        "    for i in frame_indices:\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            continue\n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        frames.append(frame)\n",
        "    cap.release()\n",
        "    return frames\n",
        "\n",
        "def preprocess_frames(frames):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToPILImage(),\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "    frames_tensor = [transform(frame) for frame in frames]\n",
        "    return torch.stack(frames_tensor)\n",
        "\n",
        "def predict_video(video):\n",
        "    frames = extract_frames(video)\n",
        "    frames_tensor = preprocess_frames(frames)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = video_model(frames_tensor)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        prediction = torch.round(torch.mean(predicted.float()))\n",
        "\n",
        "    label = \"Fake\" if prediction.item() else \"Real\"\n",
        "    confidence = 100.0  # Placeholder since we don't have softmax here\n",
        "\n",
        "    log_prediction(\"video\", label, confidence)\n",
        "\n",
        "    return f\"Video Prediction: {label}\"\n",
        "\n",
        "# Gradio interfaces\n",
        "image_interface = gr.Interface(\n",
        "    fn=predict_image,\n",
        "    inputs=gr.Image(type=\"pil\", label=\"Upload an Image\"),\n",
        "    outputs=gr.Textbox(label=\"Prediction Result\"),\n",
        "    title=\"Deepfake Image Detector\"\n",
        ")\n",
        "\n",
        "both_text_interface = gr.Interface(\n",
        "    fn=predict_both_texts,\n",
        "    inputs=gr.Textbox(lines=5, placeholder=\"Paste any caption text here...\"),\n",
        "    outputs=[\n",
        "        gr.Textbox(label=\"Original Text Model Prediction\"),\n",
        "        gr.Textbox(label=\"Generated Text Model Prediction\")\n",
        "    ],\n",
        "    title=\"Fake News Detection (Original and Generated Captions)\"\n",
        ")\n",
        "\n",
        "video_interface = gr.Interface(\n",
        "    fn=predict_video,\n",
        "    inputs=gr.Video(label=\"Upload a Video\"),\n",
        "    outputs=gr.Textbox(label=\"Prediction Result\"),\n",
        "    title=\"Deepfake Video Detector\"\n",
        ")\n",
        "\n",
        "# Launch with tabs\n",
        "gr.TabbedInterface(\n",
        "    [image_interface, both_text_interface, video_interface],\n",
        "    tab_names=[\"Detect Deepfake Image\", \"Detect Fake News (Text Captions)\", \"Detect Deepfake Video\"]\n",
        ").launch(server_name=\"0.0.0.0\")\n"
      ]
    }
  ]
}